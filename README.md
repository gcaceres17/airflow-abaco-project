# Proyecto Airflow-Abaco ETL

Proyecto de ETL usando Apache Airflow, Docker Compose y PostgreSQL para procesar **33 tablas** desde la API de Abaco.

## ğŸ“ Estructura del Proyecto

```
airflow-abaco-project/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ abaco_etl_dag.py      # DAG principal (genera tareas dinÃ¡micamente)
â”‚   â””â”€â”€ abaco_config.py       # ConfiguraciÃ³n de las 33 tablas
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ create_tables.sql     # SQL para crear las 33 tablas
â”œâ”€â”€ docker-compose.yaml       # Servicios de Airflow y PostgreSQL
â”œâ”€â”€ .env                      # Variables de entorno
â””â”€â”€ requirements.txt          # Dependencias Python
```

## ğŸš€ Inicio RÃ¡pido

### 1. Configurar tus Tablas

Edita `dags/abaco_config.py` y completa la lista `TABLES` con tus 33 tablas:

```python
TABLES = [
    {
        'name': 'clientes',
        'endpoint': 'clientes',
        'primary_key': 'id_cliente',
        'columns': ['id_cliente', 'nombre', 'email', ...]
    },
    # ... agregar las 32 tablas restantes
]
```

### 2. Crear las Tablas en PostgreSQL

Edita `scripts/create_tables.sql` con el DDL de tus 33 tablas, luego ejecuta:

```bash
docker compose up -d postgres
docker compose exec postgres psql -U airflow -d airflow -f /opt/airflow/scripts/create_tables.sql
```

### 3. Inicializar Airflow

```bash
docker compose up airflow-init
```

### 4. Iniciar los Servicios

```bash
docker compose up -d
```

### 5. Acceder a Airflow UI

- **URL**: http://localhost:8080
- **Usuario**: `airflow`
- **ContraseÃ±a**: `airflow`

### 6. Ejecutar el DAG

1. En la UI, busca `abaco_etl_dag`
2. Activa el DAG (toggle ON)
3. Ejecuta manualmente o espera la ejecuciÃ³n diaria

## ğŸ“Š Acceder a PgAdmin

- **URL**: http://localhost:5050
- **Email**: `admin@admin.com`
- **ContraseÃ±a**: `admin`

ConexiÃ³n a PostgreSQL:
- Host: `postgres`
- Puerto: `5432`
- Usuario: `airflow`
- ContraseÃ±a: `airflow`
- Base de datos: `airflow`

## ğŸ”§ CÃ³mo Funciona

El DAG usa un **patrÃ³n simple basado en configuraciÃ³n**:

1. Lee la lista de tablas desde `abaco_config.py`
2. Para cada tabla, crea 3 tareas automÃ¡ticamente:
   - **Extract**: Obtiene datos de la API de Abaco
   - **Transform**: Prepara los datos para PostgreSQL
   - **Load**: Inserta/actualiza en PostgreSQL (UPSERT)

### Agregar una Nueva Tabla

Solo necesitas:
1. Agregar la configuraciÃ³n en `abaco_config.py`
2. Agregar el `CREATE TABLE` en `create_tables.sql`
3. Reiniciar el DAG

Â¡No necesitas modificar el cÃ³digo del DAG!

## ğŸ› ï¸ Comandos Ãštiles

```bash
# Ver logs de Airflow
docker compose logs -f airflow-scheduler

# Reiniciar servicios
docker compose restart

# Detener todo
docker compose down

# Limpiar todo (incluyendo volÃºmenes)
docker compose down -v
```

## ğŸ“ Notas

- El DAG estÃ¡ configurado para ejecutarse diariamente (`@daily`)
- Usa UPSERT para evitar duplicados
- Todas las tablas tienen una columna `ingested_at` para tracking
- Los datos se extraen de `http://host.docker.internal:5001` (ajusta en `abaco_config.py`)

## ğŸ”„ PrÃ³ximos Pasos (Opcional)

Una vez que te sientas cÃ³modo, puedes mejorar el proyecto:
- Agregar foreign keys entre tablas
- Implementar carga incremental
- Agregar schemas separados (raw/staging/analytics)
- Usar TaskGroups para organizar mejor las tareas
- Implementar tests y validaciones
